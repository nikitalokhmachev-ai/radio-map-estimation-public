{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can use the button below to open this notebook in Google Colab. Note that changes made to the notebook in Colab will not be reflected in Github, nor can the notebook be saved on Colab without first making a copy. \n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nikitalokhmachev-ai/radio-map-estimation-public/blob/main/notebooks/Visualize_Inputs_Outputs.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If opened in Colab, set `using_colab` to `True` in the code block below, then run the second and (optionally) third blocks. The second block will clone the github repository into Colab's local storage in order to load the models and other functions. The third block will connect to Google Drive (user login required), which allows the Colab notebook to read and write data to the drive (e.g. training data or evaluation results)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "using_colab = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if using_colab:\n",
        "    %cd /content/\n",
        "    !rm -rf /content/radio-map-estimation-public\n",
        "    !git clone https://github.com/nikitalokhmachev-ai/radio-map-estimation-public.git\n",
        "    !pip install -q -r /content/radio-map-estimation-public/colab_requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if using_colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Untar Validation Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We visualize the inputs and outputs of the validation data, but you can use any data you choose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Jdtnv8-45Kd"
      },
      "outputs": [],
      "source": [
        "!tar -xkf '/Path/to/saved/tar/file' -C '/path/to/save/untarred/files'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWfrHtpz0pbf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import joblib\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import model architectures and data structures\n",
        "\n",
        "os.chdir('path/to/repository')\n",
        "from data_utils import MapDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Set Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set random seed, define device\n",
        "\n",
        "seed = 3\n",
        "torch.manual_seed(seed)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify folder containing trained models\n",
        "model_folder = '/folder/with/trained/models'\n",
        "\n",
        "# Specify path to untarred validation data\n",
        "val_data_folder = '/path/to/untarred/validation/data'\n",
        "\n",
        "# Specify path to data scaler and load\n",
        "scaler_path = 'scalers/minmax_scaler_zero_min134.joblib'\n",
        "with open(scaler_path, 'rb') as f:\n",
        "  scaler = joblib.load(f)\n",
        "\n",
        "# Set folder to save visualizations\n",
        "viz_folder = '/Path/to/save/visualizations'\n",
        "if not os.path.exists(viz_folder):\n",
        "    os.makedirs(viz_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualize Input and Ground Truth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Sample Map and Environment Mask are used as inputs to the model. The complete Radio Map is the ground truth that the model seeks to recreate. We use the example map shown in the paper below, but this can be replaced with any image from the validation set or other dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAtgjS37EtE-"
      },
      "outputs": [],
      "source": [
        "# Example batch\n",
        "example_batch = os.path.join(val_data_folder, 'test_0.01%_batch_0.pickle')\n",
        "# Index of map within batch\n",
        "i=37\n",
        "\n",
        "# Load batch\n",
        "t_x_points, t_channel_pows, t_y_masks = np.load(example_batch, allow_pickle=True)\n",
        "# Select map within in batch\n",
        "t_x_points = t_x_points[i:i+1]\n",
        "t_channel_pows = t_channel_pows[i:i+1]\n",
        "t_y_masks = t_y_masks[i:i+1]\n",
        "\n",
        "# Manually preprocess map (this would normally be done by the MapDataset class)\n",
        "t_y_points = t_channel_pows * t_y_masks\n",
        "t_x_masks = t_x_points[:,1,:,:] == 1\n",
        "t_x_points[:,0,:,:] = scaler.transform(t_x_points[:,0,:,:]) * t_x_masks\n",
        "t_channel_pows = scaler.transform(t_channel_pows)\n",
        "t_y_points = scaler.transform(t_y_points)\n",
        "\n",
        "sample_map = t_x_points[0,0,:,:]\n",
        "env_mask = t_x_points[0,1,:,:]\n",
        "target = t_y_points[0,0,:,:]\n",
        "target[env_mask==-1] = 1\n",
        "\n",
        "# Visualize\n",
        "fig, axs = plt.subplots(1,3, figsize=(6,5))\n",
        "axs[0].imshow(sample_map, cmap='hot', vmin=0, vmax=1)\n",
        "axs[0].set_title('Sampled Map')\n",
        "axs[1].imshow(env_mask, cmap='binary')\n",
        "axs[1].set_title('Environment Mask')\n",
        "axs[2].imshow(target, cmap='hot', vmin=0, vmax=1)\n",
        "axs[2].set_title('Complete Radio Map')\n",
        "[ax.set_xticks([]) for ax in axs]\n",
        "[ax.set_yticks([]) for ax in axs]\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualize Output and Intermediate Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model_output(x, channel_id, model, model_layer):\n",
        "  #x: bs, c, h, w\n",
        "  x = x.to(device)\n",
        "  activation = {}\n",
        "  def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "  model_layer.register_forward_hook(get_activation('out'))\n",
        "  output = model(x)\n",
        "\n",
        "  return activation['out'][0].permute(1,2,0).detach().cpu()[:,:,channel_id].unsqueeze(-1).numpy()\n",
        "\n",
        "def visualize_layer(x, model, model_layer, nrows, ncols, figsize=(15, 15), out_folder=None, filename=None):\n",
        "  n_channels = model_layer.out_channels\n",
        "  fig, axs = plt.subplots(nrows, ncols, figsize=figsize)\n",
        "  for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "      channel_id = i * ncols + j\n",
        "      if channel_id < n_channels:\n",
        "        axs[i, j].imshow(get_model_output(x, channel_id, model, model_layer))\n",
        "        axs[i, j].set_title(str(channel_id))\n",
        "        axs[i, j].axis('off')\n",
        "      else:\n",
        "        axs[i, j].axis('off')\n",
        "  plt.tight_layout()\n",
        "  if out_folder and filename:\n",
        "    plt.savefig(os.path.join(out_folder, filename))\n",
        "  plt.show()\n",
        "\n",
        "def visualize_output(x, model, figsize=(5, 5), out_folder=None, filename=None):\n",
        "  x_mask = x[:,1,:,:]\n",
        "  plt.figure(figsize=figsize)\n",
        "  plt.axis('off')\n",
        "  plt.title('Model Output')\n",
        "  prediction = model(x).reshape(1,32,32)\n",
        "  prediction[x_mask==-1] = 1\n",
        "  prediction = prediction.detach().cpu().numpy().transpose(1,2,0)\n",
        "  plt.imshow(prediction, cmap='hot', vmin=0, vmax=1)\n",
        "  plt.tight_layout()\n",
        "  if out_folder and filename:\n",
        "    plt.savefig(os.path.join(out_folder, filename))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code below visualizes either the output of the model (i.e. the predicted map) or the model representation at an intermediate layer. The user first specifies the model. If visualizing an intermediate layer, the user also specifies the layer from either the encoder or decoder. Layer names and attributes are printed out in the list below. Note that only Conv2d or ConvTranspose2d layers can be visualized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klmOrKSVwN58"
      },
      "outputs": [],
      "source": [
        "model_name = 'Baseline'\n",
        "model = torch.load(os.path.join(model_folder, f'{model_name}.pth'), weights_only=False, map_location=device)\n",
        "model.eval()\n",
        "\n",
        "encoder = model.encoder\n",
        "decoder = model.decoder\n",
        "print(encoder)\n",
        "print()\n",
        "print(decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert input numpy array to tensor\n",
        "x = torch.from_numpy(t_x_points).to(torch.float32).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select layer and visualize channel outputs\n",
        "model_layer = encoder.conv2d_1\n",
        "visualize_layer(x, model, model_layer, nrows=5, ncols=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model output\n",
        "visualize_output(x, model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
